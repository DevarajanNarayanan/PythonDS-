{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file Path\n",
    "os.chdir(\"D:\\\\DS\\\\Python DS\\\\Sample\\\\Boston Housing Data Set\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the dataset\n",
    "dataset=pd.read_csv(\"BostonHousing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=dataset.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=dataset.iloc[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crim       0\n",
       "zn         0\n",
       "indus      0\n",
       "chas       0\n",
       "nox        0\n",
       "rm         0\n",
       "age        0\n",
       "dis        0\n",
       "rad        0\n",
       "tax        0\n",
       "ptratio    0\n",
       "b          0\n",
       "lstat      0\n",
       "medv       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for null value\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg=LinearRegression()\n",
    "reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36.43982749, 20.07777254, 23.48205259, 14.4836588 , 29.83862368,\n",
       "       35.80684363, 21.17004954, 14.42905731, 29.5379636 , 12.62903377,\n",
       "       18.94618361, 32.34133827, 32.27045563, 19.11479817, 16.25842681,\n",
       "       25.66896867, 14.69888783, 16.32083624, 20.09978132, 18.12646089,\n",
       "       20.23787702, 21.5581219 , 27.62003637, 13.69234659, 21.85109312,\n",
       "       24.72816542, 21.18121808, 10.81716501, 23.48394609, 19.03461706,\n",
       "       25.33499977, 22.15000425, 23.46940052, 24.51472629, 24.36358038,\n",
       "       20.38467755, 20.15656247,  8.84083049, 20.80771367, 20.08507867,\n",
       "       28.5363048 , 24.1967635 , 13.08514888, 23.00141326, 13.81300159,\n",
       "       22.92855629, 35.75104898, 20.89199463, 26.80849405, 18.57955197,\n",
       "       31.25515593, 20.26915358, 38.00374945, 35.93611531, 30.48558935,\n",
       "       20.50488257, 31.44706178, 24.13559706, 13.43826445, 14.0615829 ,\n",
       "       18.40658051, 23.03108618, 11.42854051, 11.25602635, 17.20482644,\n",
       "       19.62850359, 41.82324602, 21.6974846 , 14.28613824, 19.68465424,\n",
       "       21.97023806, 19.07029134, 23.11471161, 18.86554262, 34.94635634,\n",
       "       20.14804032, 22.69943686, 23.06858718, 16.67684233, 33.48510112,\n",
       "       26.52547411, 28.96781824, 21.01890627, 19.72937692, 19.32720741,\n",
       "        3.27950705, 18.38122667, 15.93136345, 23.01930066, 27.75382806,\n",
       "       22.72457882, 21.56781556, 36.30818745, 22.81802265, 14.99297553,\n",
       "       34.47075939, 21.50212225, 16.17080587, 21.13741011, 18.57791739,\n",
       "       14.23470428, 21.14314432])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274    32\n",
       "500    16\n",
       "117    19\n",
       "54     18\n",
       "278    29\n",
       "166    50\n",
       "11     18\n",
       "127    16\n",
       "200    32\n",
       "134    15\n",
       "132    23\n",
       "231    31\n",
       "57     31\n",
       "310    16\n",
       "17     17\n",
       "87     22\n",
       "149    15\n",
       "474    13\n",
       "407    27\n",
       "105    19\n",
       "118    20\n",
       "121    20\n",
       "80     28\n",
       "126    15\n",
       "86     22\n",
       "5      28\n",
       "155    15\n",
       "30     12\n",
       "242    22\n",
       "420    16\n",
       "       ..\n",
       "326    23\n",
       "497    18\n",
       "273    35\n",
       "46     20\n",
       "6      22\n",
       "249    26\n",
       "458    14\n",
       "160    27\n",
       "287    23\n",
       "265    22\n",
       "487    20\n",
       "103    19\n",
       "246    24\n",
       "141    14\n",
       "9      18\n",
       "410    15\n",
       "154    17\n",
       "3      33\n",
       "38     24\n",
       "125    21\n",
       "256    44\n",
       "77     20\n",
       "378    13\n",
       "228    46\n",
       "333    22\n",
       "450    13\n",
       "58     23\n",
       "478    14\n",
       "346    17\n",
       "255    20\n",
       "Name: medv, Length: 102, dtype: int32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7713120329936831"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#R2\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.06037306e-01,  4.44284774e-02, -1.26136980e-02,  3.62716573e+00,\n",
       "       -1.77007269e+01,  3.09575485e+00, -8.83886254e-04, -1.57460330e+00,\n",
       "        3.19649339e-01, -1.22362123e-02, -1.02708132e+00,  7.54265886e-03,\n",
       "       -5.51670577e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.42290471941455"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for Adjusted R Square value\n",
    "import statsmodels.formula.api as sm\n",
    "reg_ols=sm.OLS(endog=y_train, exog=x_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.953</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.952</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   611.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 14 Sep 2019</td> <th>  Prob (F-statistic):</th> <td>1.63e-250</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:06:02</td>     <th>  Log-Likelihood:    </th> <td> -1238.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   404</td>      <th>  AIC:               </th> <td>   2502.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th> <td>   2554.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>crim</th>    <td>   -0.1013</td> <td>    0.039</td> <td>   -2.590</td> <td> 0.010</td> <td>   -0.178</td> <td>   -0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>zn</th>      <td>    0.0455</td> <td>    0.018</td> <td>    2.573</td> <td> 0.010</td> <td>    0.011</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>indus</th>   <td>   -0.0608</td> <td>    0.080</td> <td>   -0.763</td> <td> 0.446</td> <td>   -0.217</td> <td>    0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>chas</th>    <td>    3.8627</td> <td>    1.037</td> <td>    3.724</td> <td> 0.000</td> <td>    1.824</td> <td>    5.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>nox</th>     <td>    0.4710</td> <td>    4.010</td> <td>    0.117</td> <td> 0.907</td> <td>   -7.413</td> <td>    8.355</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rm</th>      <td>    5.5935</td> <td>    0.357</td> <td>   15.672</td> <td> 0.000</td> <td>    4.892</td> <td>    6.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>     <td>   -0.0117</td> <td>    0.017</td> <td>   -0.706</td> <td> 0.480</td> <td>   -0.044</td> <td>    0.021</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dis</th>     <td>   -0.9542</td> <td>    0.238</td> <td>   -4.011</td> <td> 0.000</td> <td>   -1.422</td> <td>   -0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>rad</th>     <td>    0.1710</td> <td>    0.078</td> <td>    2.181</td> <td> 0.030</td> <td>    0.017</td> <td>    0.325</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tax</th>     <td>   -0.0089</td> <td>    0.005</td> <td>   -1.897</td> <td> 0.059</td> <td>   -0.018</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ptratio</th> <td>   -0.3495</td> <td>    0.128</td> <td>   -2.720</td> <td> 0.007</td> <td>   -0.602</td> <td>   -0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>b</th>       <td>    0.0139</td> <td>    0.003</td> <td>    4.456</td> <td> 0.000</td> <td>    0.008</td> <td>    0.020</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lstat</th>   <td>   -0.4146</td> <td>    0.059</td> <td>   -7.004</td> <td> 0.000</td> <td>   -0.531</td> <td>   -0.298</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>160.679</td> <th>  Durbin-Watson:     </th> <td>   2.020</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1024.695</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.558</td>  <th>  Prob(JB):          </th> <td>3.09e-223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>10.153</td>  <th>  Cond. No.          </th> <td>8.64e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 8.64e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.953\n",
       "Model:                            OLS   Adj. R-squared:                  0.952\n",
       "Method:                 Least Squares   F-statistic:                     611.7\n",
       "Date:                Sat, 14 Sep 2019   Prob (F-statistic):          1.63e-250\n",
       "Time:                        20:06:02   Log-Likelihood:                -1238.0\n",
       "No. Observations:                 404   AIC:                             2502.\n",
       "Df Residuals:                     391   BIC:                             2554.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "crim          -0.1013      0.039     -2.590      0.010      -0.178      -0.024\n",
       "zn             0.0455      0.018      2.573      0.010       0.011       0.080\n",
       "indus         -0.0608      0.080     -0.763      0.446      -0.217       0.096\n",
       "chas           3.8627      1.037      3.724      0.000       1.824       5.902\n",
       "nox            0.4710      4.010      0.117      0.907      -7.413       8.355\n",
       "rm             5.5935      0.357     15.672      0.000       4.892       6.295\n",
       "age           -0.0117      0.017     -0.706      0.480      -0.044       0.021\n",
       "dis           -0.9542      0.238     -4.011      0.000      -1.422      -0.486\n",
       "rad            0.1710      0.078      2.181      0.030       0.017       0.325\n",
       "tax           -0.0089      0.005     -1.897      0.059      -0.018       0.000\n",
       "ptratio       -0.3495      0.128     -2.720      0.007      -0.602      -0.097\n",
       "b              0.0139      0.003      4.456      0.000       0.008       0.020\n",
       "lstat         -0.4146      0.059     -7.004      0.000      -0.531      -0.298\n",
       "==============================================================================\n",
       "Omnibus:                      160.679   Durbin-Watson:                   2.020\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1024.695\n",
       "Skew:                           1.558   Prob(JB):                    3.09e-223\n",
       "Kurtosis:                      10.153   Cond. No.                     8.64e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 8.64e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c Value missing, hence adding 1\n",
    "x_train=np.append(arr=x_train, values=np.ones((404,1)).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.append(arr=x_test, values=np.ones((102,1)).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.69311</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.376</td>\n",
       "      <td>88.4</td>\n",
       "      <td>2.5671</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>391.43</td>\n",
       "      <td>14.65</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>5.869</td>\n",
       "      <td>46.3</td>\n",
       "      <td>5.2311</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.14476</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>5.731</td>\n",
       "      <td>65.2</td>\n",
       "      <td>2.7592</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>391.50</td>\n",
       "      <td>13.61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.34284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>6.066</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.7573</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>353.89</td>\n",
       "      <td>6.43</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.03584</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3980</td>\n",
       "      <td>6.290</td>\n",
       "      <td>17.8</td>\n",
       "      <td>6.6115</td>\n",
       "      <td>4.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.67</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.19657</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>6.226</td>\n",
       "      <td>79.2</td>\n",
       "      <td>8.0555</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>376.14</td>\n",
       "      <td>10.15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.67810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>5.935</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.8206</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>68.95</td>\n",
       "      <td>34.02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.83377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>7.802</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.0407</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>389.61</td>\n",
       "      <td>1.92</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.36920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>6.567</td>\n",
       "      <td>87.3</td>\n",
       "      <td>3.6023</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>395.69</td>\n",
       "      <td>9.28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.14866</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.727</td>\n",
       "      <td>79.9</td>\n",
       "      <td>2.7778</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>394.76</td>\n",
       "      <td>9.42</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.01096</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3890</td>\n",
       "      <td>6.453</td>\n",
       "      <td>31.9</td>\n",
       "      <td>7.3073</td>\n",
       "      <td>1.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>394.72</td>\n",
       "      <td>8.23</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.04666</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>7.107</td>\n",
       "      <td>36.6</td>\n",
       "      <td>7.3090</td>\n",
       "      <td>2.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>354.31</td>\n",
       "      <td>8.61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.21038</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>6.812</td>\n",
       "      <td>32.2</td>\n",
       "      <td>4.1007</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.85</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>45.74610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>4.519</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.6582</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>88.27</td>\n",
       "      <td>36.98</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5850</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.06664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>6.546</td>\n",
       "      <td>33.1</td>\n",
       "      <td>3.1323</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>390.96</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4580</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.62356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>6.879</td>\n",
       "      <td>77.7</td>\n",
       "      <td>3.2721</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>390.39</td>\n",
       "      <td>9.93</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18.81100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>4.628</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5539</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>28.79</td>\n",
       "      <td>34.37</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.17134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>5.928</td>\n",
       "      <td>88.2</td>\n",
       "      <td>2.4631</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>344.91</td>\n",
       "      <td>15.76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.09299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.961</td>\n",
       "      <td>92.9</td>\n",
       "      <td>2.0869</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>378.09</td>\n",
       "      <td>17.93</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4330</td>\n",
       "      <td>6.982</td>\n",
       "      <td>17.7</td>\n",
       "      <td>5.4917</td>\n",
       "      <td>7.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>16.1</td>\n",
       "      <td>390.43</td>\n",
       "      <td>4.86</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.18159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>6.376</td>\n",
       "      <td>54.3</td>\n",
       "      <td>4.5404</td>\n",
       "      <td>5.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.06129</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4429</td>\n",
       "      <td>7.645</td>\n",
       "      <td>49.7</td>\n",
       "      <td>5.2119</td>\n",
       "      <td>5.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>377.07</td>\n",
       "      <td>3.01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15.57570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>5.926</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.9084</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>368.74</td>\n",
       "      <td>18.13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.01501</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>7.923</td>\n",
       "      <td>24.8</td>\n",
       "      <td>5.8850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>395.52</td>\n",
       "      <td>3.16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.07875</td>\n",
       "      <td>45.0</td>\n",
       "      <td>3.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4370</td>\n",
       "      <td>6.782</td>\n",
       "      <td>41.1</td>\n",
       "      <td>3.7886</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>393.87</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>8.24809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>7.393</td>\n",
       "      <td>99.3</td>\n",
       "      <td>2.4527</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>375.87</td>\n",
       "      <td>16.74</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>0.02055</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>6.383</td>\n",
       "      <td>35.7</td>\n",
       "      <td>9.1876</td>\n",
       "      <td>2.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>17.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.77</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0.38214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>8.040</td>\n",
       "      <td>86.5</td>\n",
       "      <td>3.2157</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>387.38</td>\n",
       "      <td>3.13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>6.39312</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>6.162</td>\n",
       "      <td>97.4</td>\n",
       "      <td>2.2060</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>302.76</td>\n",
       "      <td>24.10</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>13.07510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5800</td>\n",
       "      <td>5.713</td>\n",
       "      <td>56.7</td>\n",
       "      <td>2.8237</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.76</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>15.86030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>5.896</td>\n",
       "      <td>95.4</td>\n",
       "      <td>1.9096</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>7.68</td>\n",
       "      <td>24.39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0.03659</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>6.302</td>\n",
       "      <td>32.2</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.72</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0.10328</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>5.927</td>\n",
       "      <td>47.2</td>\n",
       "      <td>6.9320</td>\n",
       "      <td>8.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>19.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.22</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>3.67822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>5.362</td>\n",
       "      <td>96.2</td>\n",
       "      <td>2.1036</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>380.79</td>\n",
       "      <td>10.19</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>24.39380</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>4.652</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>28.28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>11.10810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6680</td>\n",
       "      <td>4.906</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.1742</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>34.77</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0.04932</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4720</td>\n",
       "      <td>6.849</td>\n",
       "      <td>70.3</td>\n",
       "      <td>3.1827</td>\n",
       "      <td>7.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.53</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0.35809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5070</td>\n",
       "      <td>6.951</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.8617</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>391.70</td>\n",
       "      <td>9.71</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0.16760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>6.426</td>\n",
       "      <td>52.3</td>\n",
       "      <td>4.5404</td>\n",
       "      <td>5.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>6.44405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>6.425</td>\n",
       "      <td>74.8</td>\n",
       "      <td>2.2004</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>97.95</td>\n",
       "      <td>12.03</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>0.04011</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4040</td>\n",
       "      <td>7.287</td>\n",
       "      <td>34.1</td>\n",
       "      <td>7.3090</td>\n",
       "      <td>2.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0.04203</td>\n",
       "      <td>28.0</td>\n",
       "      <td>15.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4640</td>\n",
       "      <td>6.442</td>\n",
       "      <td>53.6</td>\n",
       "      <td>3.6659</td>\n",
       "      <td>4.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>395.01</td>\n",
       "      <td>8.16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0.01439</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4010</td>\n",
       "      <td>6.604</td>\n",
       "      <td>18.8</td>\n",
       "      <td>6.2196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>376.70</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.26363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>6.229</td>\n",
       "      <td>91.2</td>\n",
       "      <td>2.5451</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>391.23</td>\n",
       "      <td>15.55</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.12932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4370</td>\n",
       "      <td>6.678</td>\n",
       "      <td>31.1</td>\n",
       "      <td>5.9604</td>\n",
       "      <td>4.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.27</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>9.91655</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6930</td>\n",
       "      <td>5.852</td>\n",
       "      <td>77.8</td>\n",
       "      <td>1.5004</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>338.16</td>\n",
       "      <td>29.97</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.34109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>6.415</td>\n",
       "      <td>40.1</td>\n",
       "      <td>4.7211</td>\n",
       "      <td>5.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>6.12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.06127</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4470</td>\n",
       "      <td>6.826</td>\n",
       "      <td>27.6</td>\n",
       "      <td>4.8628</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>393.45</td>\n",
       "      <td>4.16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>4.75237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.525</td>\n",
       "      <td>86.5</td>\n",
       "      <td>2.4358</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>50.92</td>\n",
       "      <td>18.13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5730</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.08014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>5.850</td>\n",
       "      <td>41.5</td>\n",
       "      <td>3.9342</td>\n",
       "      <td>5.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.77</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>5.44114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.655</td>\n",
       "      <td>98.2</td>\n",
       "      <td>2.3552</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>355.29</td>\n",
       "      <td>17.73</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows  14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0     1      2    3       4      5      6       7     8      9   \\\n",
       "0     3.69311   0.0  18.10  0.0  0.7130  6.376   88.4  2.5671  24.0  666.0   \n",
       "1     0.62739   0.0   8.14  0.0  0.5380  5.834   56.5  4.4986   4.0  307.0   \n",
       "2     0.03427   0.0   5.19  0.0  0.5150  5.869   46.3  5.2311   5.0  224.0   \n",
       "3     0.14476   0.0  10.01  0.0  0.5470  5.731   65.2  2.7592   6.0  432.0   \n",
       "4     1.34284   0.0  19.58  0.0  0.6050  6.066  100.0  1.7573   5.0  403.0   \n",
       "5     0.03584  80.0   3.37  0.0  0.3980  6.290   17.8  6.6115   4.0  337.0   \n",
       "6     0.19657  22.0   5.86  0.0  0.4310  6.226   79.2  8.0555   7.0  330.0   \n",
       "7    13.67810   0.0  18.10  0.0  0.7400  5.935   87.9  1.8206  24.0  666.0   \n",
       "8     1.83377   0.0  19.58  1.0  0.6050  7.802   98.2  2.0407   5.0  403.0   \n",
       "9     0.36920   0.0   9.90  0.0  0.5440  6.567   87.3  3.6023   4.0  304.0   \n",
       "10    0.14866   0.0   8.56  0.0  0.5200  6.727   79.9  2.7778   5.0  384.0   \n",
       "11    0.01096  55.0   2.25  0.0  0.3890  6.453   31.9  7.3073   1.0  300.0   \n",
       "12    0.04666  80.0   1.52  0.0  0.4040  7.107   36.6  7.3090   2.0  329.0   \n",
       "13    0.21038  20.0   3.33  0.0  0.4429  6.812   32.2  4.1007   5.0  216.0   \n",
       "14   45.74610   0.0  18.10  0.0  0.6930  4.519  100.0  1.6582  24.0  666.0   \n",
       "15    0.17331   0.0   9.69  0.0  0.5850  5.707   54.0  2.3817   6.0  391.0   \n",
       "16    0.06664   0.0   4.05  0.0  0.5100  6.546   33.1  3.1323   5.0  296.0   \n",
       "17    0.06905   0.0   2.18  0.0  0.4580  7.147   54.2  6.0622   3.0  222.0   \n",
       "18    0.62356   0.0   6.20  1.0  0.5070  6.879   77.7  3.2721   8.0  307.0   \n",
       "19   18.81100   0.0  18.10  0.0  0.5970  4.628  100.0  1.5539  24.0  666.0   \n",
       "20    0.17134   0.0  10.01  0.0  0.5470  5.928   88.2  2.4631   6.0  432.0   \n",
       "21    0.09299   0.0  25.65  0.0  0.5810  5.961   92.9  2.0869   2.0  188.0   \n",
       "22    0.10000  34.0   6.09  0.0  0.4330  6.982   17.7  5.4917   7.0  329.0   \n",
       "23    3.67367   0.0  18.10  0.0  0.5830  6.312   51.9  3.9917  24.0  666.0   \n",
       "24    0.18159   0.0   7.38  0.0  0.4930  6.376   54.3  4.5404   5.0  287.0   \n",
       "25    0.06129  20.0   3.33  1.0  0.4429  7.645   49.7  5.2119   5.0  216.0   \n",
       "26   15.57570   0.0  18.10  0.0  0.5800  5.926   71.0  2.9084  24.0  666.0   \n",
       "27    0.04741   0.0  11.93  0.0  0.5730  6.030   80.8  2.5050   1.0  273.0   \n",
       "28    0.01501  90.0   1.21  1.0  0.4010  7.923   24.8  5.8850   1.0  198.0   \n",
       "29    0.07875  45.0   3.44  0.0  0.4370  6.782   41.1  3.7886   5.0  398.0   \n",
       "..        ...   ...    ...  ...     ...    ...    ...     ...   ...    ...   \n",
       "374   8.24809   0.0  18.10  0.0  0.7130  7.393   99.3  2.4527  24.0  666.0   \n",
       "375   0.02055  85.0   0.74  0.0  0.4100  6.383   35.7  9.1876   2.0  313.0   \n",
       "376   0.38214   0.0   6.20  0.0  0.5040  8.040   86.5  3.2157   8.0  307.0   \n",
       "377   6.39312   0.0  18.10  0.0  0.5840  6.162   97.4  2.2060  24.0  666.0   \n",
       "378   0.85204   0.0   8.14  0.0  0.5380  5.965   89.2  4.0123   4.0  307.0   \n",
       "379  13.07510   0.0  18.10  0.0  0.5800  5.713   56.7  2.8237  24.0  666.0   \n",
       "380  15.86030   0.0  18.10  0.0  0.6790  5.896   95.4  1.9096  24.0  666.0   \n",
       "381   0.20746   0.0  27.74  0.0  0.6090  5.093   98.0  1.8226   4.0  711.0   \n",
       "382   0.03659  25.0   4.86  0.0  0.4260  6.302   32.2  5.4007   4.0  281.0   \n",
       "383   0.10328  25.0   5.13  0.0  0.4530  5.927   47.2  6.9320   8.0  284.0   \n",
       "384   3.67822   0.0  18.10  0.0  0.7700  5.362   96.2  2.1036  24.0  666.0   \n",
       "385  24.39380   0.0  18.10  0.0  0.7000  4.652  100.0  1.4672  24.0  666.0   \n",
       "386  11.10810   0.0  18.10  0.0  0.6680  4.906  100.0  1.1742  24.0  666.0   \n",
       "387   0.04932  33.0   2.18  0.0  0.4720  6.849   70.3  3.1827   7.0  222.0   \n",
       "388   0.35809   0.0   6.20  1.0  0.5070  6.951   88.5  2.8617   8.0  307.0   \n",
       "389   0.16760   0.0   7.38  0.0  0.4930  6.426   52.3  4.5404   5.0  287.0   \n",
       "390   6.44405   0.0  18.10  0.0  0.5840  6.425   74.8  2.2004  24.0  666.0   \n",
       "391   0.04011  80.0   1.52  0.0  0.4040  7.287   34.1  7.3090   2.0  329.0   \n",
       "392   0.04203  28.0  15.04  0.0  0.4640  6.442   53.6  3.6659   4.0  270.0   \n",
       "393   0.01439  60.0   2.93  0.0  0.4010  6.604   18.8  6.2196   1.0  265.0   \n",
       "394   0.26363   0.0   8.56  0.0  0.5200  6.229   91.2  2.5451   5.0  384.0   \n",
       "395   0.12932   0.0  13.92  0.0  0.4370  6.678   31.1  5.9604   4.0  289.0   \n",
       "396   9.91655   0.0  18.10  0.0  0.6930  5.852   77.8  1.5004  24.0  666.0   \n",
       "397   0.34109   0.0   7.38  0.0  0.4930  6.415   40.1  4.7211   5.0  287.0   \n",
       "398   0.06127  40.0   6.41  1.0  0.4470  6.826   27.6  4.8628   4.0  254.0   \n",
       "399   0.06076   0.0  11.93  0.0  0.5730  6.976   91.0  2.1675   1.0  273.0   \n",
       "400   4.75237   0.0  18.10  0.0  0.7130  6.525   86.5  2.4358  24.0  666.0   \n",
       "401   0.04527   0.0  11.93  0.0  0.5730  6.120   76.7  2.2875   1.0  273.0   \n",
       "402   0.08014   0.0   5.96  0.0  0.4990  5.850   41.5  3.9342   5.0  279.0   \n",
       "403   5.44114   0.0  18.10  0.0  0.7130  6.655   98.2  2.3552  24.0  666.0   \n",
       "\n",
       "       10      11     12   13  \n",
       "0    20.2  391.43  14.65  1.0  \n",
       "1    21.0  395.62   8.47  1.0  \n",
       "2    20.2  396.90   9.80  1.0  \n",
       "3    17.8  391.50  13.61  1.0  \n",
       "4    14.7  353.89   6.43  1.0  \n",
       "5    16.1  396.90   4.67  1.0  \n",
       "6    19.1  376.14  10.15  1.0  \n",
       "7    20.2   68.95  34.02  1.0  \n",
       "8    14.7  389.61   1.92  1.0  \n",
       "9    18.4  395.69   9.28  1.0  \n",
       "10   20.9  394.76   9.42  1.0  \n",
       "11   15.3  394.72   8.23  1.0  \n",
       "12   12.6  354.31   8.61  1.0  \n",
       "13   14.9  396.90   4.85  1.0  \n",
       "14   20.2   88.27  36.98  1.0  \n",
       "15   19.2  396.90  12.01  1.0  \n",
       "16   16.6  390.96   5.33  1.0  \n",
       "17   18.7  396.90   5.33  1.0  \n",
       "18   17.4  390.39   9.93  1.0  \n",
       "19   20.2   28.79  34.37  1.0  \n",
       "20   17.8  344.91  15.76  1.0  \n",
       "21   19.1  378.09  17.93  1.0  \n",
       "22   16.1  390.43   4.86  1.0  \n",
       "23   20.2  388.62  10.58  1.0  \n",
       "24   19.6  396.90   6.87  1.0  \n",
       "25   14.9  377.07   3.01  1.0  \n",
       "26   20.2  368.74  18.13  1.0  \n",
       "27   21.0  396.90   7.88  1.0  \n",
       "28   13.6  395.52   3.16  1.0  \n",
       "29   15.2  393.87   6.68  1.0  \n",
       "..    ...     ...    ...  ...  \n",
       "374  20.2  375.87  16.74  1.0  \n",
       "375  17.3  396.90   5.77  1.0  \n",
       "376  17.4  387.38   3.13  1.0  \n",
       "377  20.2  302.76  24.10  1.0  \n",
       "378  21.0  392.53  13.83  1.0  \n",
       "379  20.2  396.90  14.76  1.0  \n",
       "380  20.2    7.68  24.39  1.0  \n",
       "381  20.1  318.43  29.68  1.0  \n",
       "382  19.0  396.90   6.72  1.0  \n",
       "383  19.7  396.90   9.22  1.0  \n",
       "384  20.2  380.79  10.19  1.0  \n",
       "385  20.2  396.90  28.28  1.0  \n",
       "386  20.2  396.90  34.77  1.0  \n",
       "387  18.4  396.90   7.53  1.0  \n",
       "388  17.4  391.70   9.71  1.0  \n",
       "389  19.6  396.90   7.20  1.0  \n",
       "390  20.2   97.95  12.03  1.0  \n",
       "391  12.6  396.90   4.08  1.0  \n",
       "392  18.2  395.01   8.16  1.0  \n",
       "393  15.6  376.70   4.38  1.0  \n",
       "394  20.9  391.23  15.55  1.0  \n",
       "395  16.0  396.90   6.27  1.0  \n",
       "396  20.2  338.16  29.97  1.0  \n",
       "397  19.6  396.90   6.12  1.0  \n",
       "398  17.6  393.45   4.16  1.0  \n",
       "399  21.0  396.90   5.64  1.0  \n",
       "400  20.2   50.92  18.13  1.0  \n",
       "401  21.0  396.90   9.08  1.0  \n",
       "402  19.2  396.90   8.77  1.0  \n",
       "403  20.2  355.29  17.73  1.0  \n",
       "\n",
       "[404 rows x 14 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ols = sm.OLS(endog=y_train, exog=x_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>medv</td>       <th>  R-squared:         </th> <td>   0.728</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.719</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   80.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 14 Sep 2019</td> <th>  Prob (F-statistic):</th> <td>1.37e-101</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>20:06:03</td>     <th>  Log-Likelihood:    </th> <td> -1211.6</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   404</td>      <th>  AIC:               </th> <td>   2451.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   390</td>      <th>  BIC:               </th> <td>   2507.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    13</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.1060</td> <td>    0.037</td> <td>   -2.889</td> <td> 0.004</td> <td>   -0.178</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0444</td> <td>    0.017</td> <td>    2.680</td> <td> 0.008</td> <td>    0.012</td> <td>    0.077</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0126</td> <td>    0.075</td> <td>   -0.168</td> <td> 0.867</td> <td>   -0.160</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    3.6272</td> <td>    0.973</td> <td>    3.726</td> <td> 0.000</td> <td>    1.713</td> <td>    5.541</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>  -17.7007</td> <td>    4.495</td> <td>   -3.937</td> <td> 0.000</td> <td>  -26.539</td> <td>   -8.862</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    3.0958</td> <td>    0.476</td> <td>    6.503</td> <td> 0.000</td> <td>    2.160</td> <td>    4.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>   -0.0009</td> <td>    0.016</td> <td>   -0.057</td> <td> 0.955</td> <td>   -0.032</td> <td>    0.030</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>   -1.5746</td> <td>    0.238</td> <td>   -6.603</td> <td> 0.000</td> <td>   -2.043</td> <td>   -1.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>    0.3196</td> <td>    0.076</td> <td>    4.194</td> <td> 0.000</td> <td>    0.170</td> <td>    0.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0122</td> <td>    0.004</td> <td>   -2.772</td> <td> 0.006</td> <td>   -0.021</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>   -1.0271</td> <td>    0.151</td> <td>   -6.780</td> <td> 0.000</td> <td>   -1.325</td> <td>   -0.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.0075</td> <td>    0.003</td> <td>    2.478</td> <td> 0.014</td> <td>    0.002</td> <td>    0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>   -0.5517</td> <td>    0.059</td> <td>   -9.423</td> <td> 0.000</td> <td>   -0.667</td> <td>   -0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   43.4229</td> <td>    5.885</td> <td>    7.379</td> <td> 0.000</td> <td>   31.854</td> <td>   54.992</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>138.118</td> <th>  Durbin-Watson:     </th> <td>   2.023</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 556.593</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.469</td>  <th>  Prob(JB):          </th> <td>1.37e-121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 7.943</td>  <th>  Cond. No.          </th> <td>1.52e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.52e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   medv   R-squared:                       0.728\n",
       "Model:                            OLS   Adj. R-squared:                  0.719\n",
       "Method:                 Least Squares   F-statistic:                     80.33\n",
       "Date:                Sat, 14 Sep 2019   Prob (F-statistic):          1.37e-101\n",
       "Time:                        20:06:03   Log-Likelihood:                -1211.6\n",
       "No. Observations:                 404   AIC:                             2451.\n",
       "Df Residuals:                     390   BIC:                             2507.\n",
       "Df Model:                          13                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1            -0.1060      0.037     -2.889      0.004      -0.178      -0.034\n",
       "x2             0.0444      0.017      2.680      0.008       0.012       0.077\n",
       "x3            -0.0126      0.075     -0.168      0.867      -0.160       0.135\n",
       "x4             3.6272      0.973      3.726      0.000       1.713       5.541\n",
       "x5           -17.7007      4.495     -3.937      0.000     -26.539      -8.862\n",
       "x6             3.0958      0.476      6.503      0.000       2.160       4.032\n",
       "x7            -0.0009      0.016     -0.057      0.955      -0.032       0.030\n",
       "x8            -1.5746      0.238     -6.603      0.000      -2.043      -1.106\n",
       "x9             0.3196      0.076      4.194      0.000       0.170       0.470\n",
       "x10           -0.0122      0.004     -2.772      0.006      -0.021      -0.004\n",
       "x11           -1.0271      0.151     -6.780      0.000      -1.325      -0.729\n",
       "x12            0.0075      0.003      2.478      0.014       0.002       0.014\n",
       "x13           -0.5517      0.059     -9.423      0.000      -0.667      -0.437\n",
       "const         43.4229      5.885      7.379      0.000      31.854      54.992\n",
       "==============================================================================\n",
       "Omnibus:                      138.118   Durbin-Watson:                   2.023\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              556.593\n",
       "Skew:                           1.469   Prob(JB):                    1.37e-121\n",
       "Kurtosis:                       7.943   Cond. No.                     1.52e+04\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.52e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "reg_tree=DecisionTreeRegressor()\n",
    "reg_tree.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree=reg_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8469222276104016"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=None,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg_ran=RandomForestRegressor(n_estimators=4)\n",
    "reg_ran.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ran=reg_ran.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259666675938097"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred_ran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0\n",
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "4     0\n",
       "5     0\n",
       "6     0\n",
       "7     0\n",
       "8     0\n",
       "9     0\n",
       "10    0\n",
       "11    0\n",
       "12    0\n",
       "13    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_train).isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\DS\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logmodel=LogisticRegression()\n",
    "logmodel.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log=logmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13725490196078433"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "class_knn=KNeighborsClassifier(n_neighbors=5, metric='minkowski',p=2)\n",
    "class_knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn=class_knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test, y_pred_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "class_nb=GaussianNB()\n",
    "class_nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_nb=class_nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06862745098039216"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM - Linear\n",
    "from sklearn.svm import SVC\n",
    "class_linear=SVC(kernel='linear')\n",
    "class_linear.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_linear=class_linear.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10784313725490197"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test, y_pred_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DS\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM - RBF\n",
    "from sklearn.svm import SVC\n",
    "class_rbf=SVC(kernel='rbf')\n",
    "class_rbf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rbf=class_rbf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058823529411764705"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test, y_pred_rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DS\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='sigmoid', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM - Sigmoid\n",
    "from sklearn.svm import SVC\n",
    "class_sigmoid=SVC(kernel='sigmoid')\n",
    "class_sigmoid.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_sigmoid=class_sigmoid.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.049019607843137254"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "accuracy_score(y_test, y_pred_sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # #Entropy - Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class_tree=DecisionTreeClassifier(criterion=\"entropy\")\n",
    "class_tree.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree=class_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13725490196078433"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=3, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "class_random = RandomForestClassifier(n_estimators=3, criterion=\"entropy\")\n",
    "class_random.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_random=class_random.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18627450980392157"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmlog=confusion_matrix(y_test, y_pred_log)\n",
    "cmknn=confusion_matrix(y_test, y_pred_knn)\n",
    "cmnb=confusion_matrix(y_test, y_pred_nb)\n",
    "cmlinear=confusion_matrix(y_test, y_pred_linear)\n",
    "cmrbf=confusion_matrix(y_test, y_pred_rbf)\n",
    "cmsigmoid=confusion_matrix(y_test, y_pred_sigmoid)\n",
    "cmtree=confusion_matrix(y_test, y_pred_tree)\n",
    "cmrandom=confusion_matrix(y_test, y_pred_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00980392156862745"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmlog[0,0]+cmlog[1,1])/sum(sum(cmlog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00980392156862745"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmknn[0,0]+cmknn[1,1])/sum(sum(cmknn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmnb[0,0]+cmnb[1,1])/sum(sum(cmnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00980392156862745"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmlinear[0,0]+cmlinear[1,1])/sum(sum(cmlinear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmrbf[0,0]+cmrbf[1,1])/sum(sum(cmrbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmsigmoid[0,0]+cmsigmoid[1,1])/sum(sum(cmsigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmtree[0,0]+cmtree[1,1])/sum(sum(cmtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cmrandom[0,0]+cmrandom[1,1])/sum(sum(cmrandom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
